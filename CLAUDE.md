Always test your changes by running the appropriate script or CLI command. Never complete a task without testing your changes until the script or CLI command runs without issues for 3 minutes+ (at minimum). If you find an error unrelated to your task, at minimum quote the exact error back to me when you have completed your task and offer to investigate and fix it.

## Project Structure and Conventions

Consider writing a new file if you add a standalone, complex feature used in more than one place.

When you write a script that launches a CLI command via a subprocess, print the CLI command so it can be easily reproduced.

Use dataclasses for config, and use simple_parsing to parse the CLI configs dataclasses. Never call a config class `cfg`, always something specific like foo_cfg, e.g. run_cfg/RunConfig. Arguments should use underscores and not dashes like `--example_arg`.

Never save logs, scripts, and other random development into the root of a project. Create an appropriate directory such as runs/ or scripts/ and add it to the .gitignore if it has only transient value.

torch.cuda.empty_cache() doesn't do what you hope it will do - don't use it.

Put imports at the top of the file unless you have a very strong need to do otherwise.

# Development

Use `pre-commit run --all-files` if you forget to install pre-commit and it doesn't run in the hook.

Open a dedicated tmux pane named "claude-commands" to run your commands so the user can monitor it.

Don't keep default run path values inside low level code - if a module calls another module, the higher level module should always inject the base path.

Don't save datasetes to a directory that is not in the gitignore.

When you follow project conventions don't leave a comment saying (following project conventions) or similar drivel. More broadly, don't centre yourself or your decisions in the codebase, and only leave comments that are useful to other users.

### Tests & Evals

Mark tests requiring GPUs with `@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")`.

To run the custom WMDP bio subset evals the path must be included. Here's a programmatic example, although this can also be done from the command line:

```
# Setup Tasks
include_path = "/path/to/bergson/bergson/unlearn/lm_eval_tasks"
tm = TaskManager(verbosity="INFO", include_path=include_path)
results = simple_evaluate(
    model=lm,
    tasks=["wmdp_bio_robust"],
    task_manager=tm,
)
```

When you run the LM eval harness ensure you use all available GPUs. You may need to launch a script using `subprocess` and capture its output.

When you hyperparameter tune don't add more training steps without permission.

When you hyperparameter tune add the settings you test and their results to a markdown file in a experiment_logs directory that you commit regularly. Be concise. Add the settings first then result when it comes in.

### Environment Setup

If you use need to use a venv, create and/or activate it with `python3 -m venv .venv && source .venv/bin/activate`.

You can pull secrets from .env.
